{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import data_processing\n",
    "from data_processing import next_batch\n",
    "from network_module import NeuralNet,NeuralNet_multilayerencode\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above data_prrocessing and network_module are self made module for this task and is available in the repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the same dataset and preprocessing that I have applied earlier for the same task using keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataread = data_processing.Datareading(dataframe = 'dataset.csv')\n",
    "Independent_data,Response = dataread.dataprocessed()\n",
    "scaler,df  = data_processing.__Processing__(df = Independent_data,process='min-max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   8.44035463e-02,   0.00000000e+00,\n",
       "          7.76475734e-05,   2.35309258e-02,   1.65677595e-02,\n",
       "          1.83312544e-02,   1.84915258e-03,   1.71414480e-02,\n",
       "          3.94041229e-02,   2.63142874e-02,   2.46140780e-02,\n",
       "          2.49297844e-02,   8.97226191e-02],\n",
       "       [  1.76149375e-04,   8.35840012e-02,   0.00000000e+00,\n",
       "          0.00000000e+00,   2.30505344e-02,   1.40488170e-02,\n",
       "          1.93644116e-02,   6.14204447e-03,   1.87181926e-02,\n",
       "          3.95184737e-02,   2.60509003e-02,   2.44917674e-02,\n",
       "          2.51390123e-02,   9.50835235e-02],\n",
       "       [  3.52298749e-04,   8.19610704e-02,   3.09318131e-04,\n",
       "          7.66195632e-05,   2.23002404e-02,   1.35873757e-02,\n",
       "          1.72959379e-02,   1.01600460e-02,   1.89700008e-02,\n",
       "          3.78068673e-02,   2.47860539e-02,   2.32636539e-02,\n",
       "          2.39265031e-02,   9.78196153e-02],\n",
       "       [  5.28448124e-04,   8.02145934e-02,   1.51759208e-03,\n",
       "          3.56653896e-04,   2.24039668e-02,   1.03798630e-02,\n",
       "          1.74822425e-02,   1.36920444e-02,   1.94957404e-02,\n",
       "          3.54017419e-02,   2.51193370e-02,   2.36983760e-02,\n",
       "          2.42847061e-02,   1.01966689e-01],\n",
       "       [  7.04597499e-04,   7.94618193e-02,   4.83309580e-03,\n",
       "          1.11709705e-03,   2.27523357e-02,   6.42186677e-03,\n",
       "          1.69577212e-02,   3.33537718e-02,   2.59721974e-02,\n",
       "          3.41758391e-02,   2.12649776e-02,   2.01561887e-02,\n",
       "          2.08754476e-02,   1.01722662e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Since I am doing pytorch for the first time, I was too excited regarding the processing step and how it is performing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used training - validation split to check the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking first 5000 rows for training and others for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Neural  nets used to learn after each batch execution then abnormal batch will not result in robust minimization, here after each learning step I am using validation error for the entire set instead of ant batch to get the better picture of real world learning through this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainx = df[:5000,:]\n",
    "testX  = df[5000:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have designed two Neural Nets, one is single layer encoder and other one is multi layer encoder. Using Multilayer for the execution. If we use single layer the other code base will remain constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNet_multilayerencode()\n",
    "net.double()\n",
    "criterion = nn.MSELoss(size_average=True)\n",
    "optimizer = optim.Adadelta(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set parameters for deep learning execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to record our training and validation losses for performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, trainingloss: 0.0141300000,entirevalidationloss: 0.0188100000\n",
      "Epoch: 1, trainingloss: 0.0141200000,entirevalidationloss: 0.0188100000\n",
      "Epoch: 2, trainingloss: 0.0141200000,entirevalidationloss: 0.0188000000\n",
      "Epoch: 3, trainingloss: 0.0141200000,entirevalidationloss: 0.0188000000\n",
      "Epoch: 4, trainingloss: 0.0141200000,entirevalidationloss: 0.0188000000\n",
      "Epoch: 5, trainingloss: 0.0141100000,entirevalidationloss: 0.0188000000\n",
      "Epoch: 6, trainingloss: 0.0141100000,entirevalidationloss: 0.0187900000\n",
      "Epoch: 7, trainingloss: 0.0141100000,entirevalidationloss: 0.0187900000\n",
      "Epoch: 8, trainingloss: 0.0141000000,entirevalidationloss: 0.0187900000\n",
      "Epoch: 9, trainingloss: 0.0141000000,entirevalidationloss: 0.0187800000\n",
      "Epoch: 10, trainingloss: 0.0141000000,entirevalidationloss: 0.0187800000\n",
      "Epoch: 11, trainingloss: 0.0141000000,entirevalidationloss: 0.0187800000\n",
      "Epoch: 12, trainingloss: 0.0140900000,entirevalidationloss: 0.0187800000\n",
      "Epoch: 13, trainingloss: 0.0140900000,entirevalidationloss: 0.0187700000\n",
      "Epoch: 14, trainingloss: 0.0140900000,entirevalidationloss: 0.0187700000\n",
      "Epoch: 15, trainingloss: 0.0140800000,entirevalidationloss: 0.0187700000\n",
      "Epoch: 16, trainingloss: 0.0140800000,entirevalidationloss: 0.0187700000\n",
      "Epoch: 17, trainingloss: 0.0140800000,entirevalidationloss: 0.0187600000\n",
      "Epoch: 18, trainingloss: 0.0140800000,entirevalidationloss: 0.0187600000\n",
      "Epoch: 19, trainingloss: 0.0140700000,entirevalidationloss: 0.0187600000\n",
      "Epoch: 20, trainingloss: 0.0140700000,entirevalidationloss: 0.0187500000\n",
      "Epoch: 21, trainingloss: 0.0140700000,entirevalidationloss: 0.0187500000\n",
      "Epoch: 22, trainingloss: 0.0140700000,entirevalidationloss: 0.0187500000\n",
      "Epoch: 23, trainingloss: 0.0140600000,entirevalidationloss: 0.0187500000\n",
      "Epoch: 24, trainingloss: 0.0140600000,entirevalidationloss: 0.0187400000\n",
      "Epoch: 25, trainingloss: 0.0140600000,entirevalidationloss: 0.0187400000\n",
      "Epoch: 26, trainingloss: 0.0140500000,entirevalidationloss: 0.0187400000\n",
      "Epoch: 27, trainingloss: 0.0140500000,entirevalidationloss: 0.0187400000\n",
      "Epoch: 28, trainingloss: 0.0140500000,entirevalidationloss: 0.0187300000\n",
      "Epoch: 29, trainingloss: 0.0140500000,entirevalidationloss: 0.0187300000\n",
      "Epoch: 30, trainingloss: 0.0140400000,entirevalidationloss: 0.0187300000\n",
      "Epoch: 31, trainingloss: 0.0140400000,entirevalidationloss: 0.0187300000\n",
      "Epoch: 32, trainingloss: 0.0140400000,entirevalidationloss: 0.0187200000\n",
      "Epoch: 33, trainingloss: 0.0140400000,entirevalidationloss: 0.0187200000\n",
      "Epoch: 34, trainingloss: 0.0140300000,entirevalidationloss: 0.0187200000\n",
      "Epoch: 35, trainingloss: 0.0140300000,entirevalidationloss: 0.0187200000\n",
      "Epoch: 36, trainingloss: 0.0140300000,entirevalidationloss: 0.0187100000\n",
      "Epoch: 37, trainingloss: 0.0140300000,entirevalidationloss: 0.0187100000\n",
      "Epoch: 38, trainingloss: 0.0140200000,entirevalidationloss: 0.0187100000\n",
      "Epoch: 39, trainingloss: 0.0140200000,entirevalidationloss: 0.0187100000\n",
      "Epoch: 40, trainingloss: 0.0140200000,entirevalidationloss: 0.0187000000\n",
      "Epoch: 41, trainingloss: 0.0140200000,entirevalidationloss: 0.0187000000\n",
      "Epoch: 42, trainingloss: 0.0140200000,entirevalidationloss: 0.0187000000\n",
      "Epoch: 43, trainingloss: 0.0140100000,entirevalidationloss: 0.0187000000\n",
      "Epoch: 44, trainingloss: 0.0140100000,entirevalidationloss: 0.0186900000\n",
      "Epoch: 45, trainingloss: 0.0140100000,entirevalidationloss: 0.0186900000\n",
      "Epoch: 46, trainingloss: 0.0140100000,entirevalidationloss: 0.0186900000\n",
      "Epoch: 47, trainingloss: 0.0140000000,entirevalidationloss: 0.0186900000\n",
      "Epoch: 48, trainingloss: 0.0140000000,entirevalidationloss: 0.0186800000\n",
      "Epoch: 49, trainingloss: 0.0140000000,entirevalidationloss: 0.0186800000\n",
      "Epoch: 50, trainingloss: 0.0140000000,entirevalidationloss: 0.0186800000\n",
      "Epoch: 51, trainingloss: 0.0139900000,entirevalidationloss: 0.0186800000\n",
      "Epoch: 52, trainingloss: 0.0139900000,entirevalidationloss: 0.0186700000\n",
      "Epoch: 53, trainingloss: 0.0139900000,entirevalidationloss: 0.0186700000\n",
      "Epoch: 54, trainingloss: 0.0139900000,entirevalidationloss: 0.0186700000\n",
      "Epoch: 55, trainingloss: 0.0139800000,entirevalidationloss: 0.0186700000\n",
      "Epoch: 56, trainingloss: 0.0139800000,entirevalidationloss: 0.0186700000\n",
      "Epoch: 57, trainingloss: 0.0139800000,entirevalidationloss: 0.0186600000\n",
      "Epoch: 58, trainingloss: 0.0139800000,entirevalidationloss: 0.0186600000\n",
      "Epoch: 59, trainingloss: 0.0139800000,entirevalidationloss: 0.0186600000\n",
      "Epoch: 60, trainingloss: 0.0139700000,entirevalidationloss: 0.0186600000\n",
      "Epoch: 61, trainingloss: 0.0139700000,entirevalidationloss: 0.0186500000\n",
      "Epoch: 62, trainingloss: 0.0139700000,entirevalidationloss: 0.0186500000\n",
      "Epoch: 63, trainingloss: 0.0139700000,entirevalidationloss: 0.0186500000\n",
      "Epoch: 64, trainingloss: 0.0139700000,entirevalidationloss: 0.0186500000\n",
      "Epoch: 65, trainingloss: 0.0139600000,entirevalidationloss: 0.0186500000\n",
      "Epoch: 66, trainingloss: 0.0139600000,entirevalidationloss: 0.0186400000\n",
      "Epoch: 67, trainingloss: 0.0139600000,entirevalidationloss: 0.0186400000\n",
      "Epoch: 68, trainingloss: 0.0139600000,entirevalidationloss: 0.0186400000\n",
      "Epoch: 69, trainingloss: 0.0139500000,entirevalidationloss: 0.0186400000\n",
      "Epoch: 70, trainingloss: 0.0139500000,entirevalidationloss: 0.0186300000\n",
      "Epoch: 71, trainingloss: 0.0139500000,entirevalidationloss: 0.0186300000\n",
      "Epoch: 72, trainingloss: 0.0139500000,entirevalidationloss: 0.0186300000\n",
      "Epoch: 73, trainingloss: 0.0139500000,entirevalidationloss: 0.0186300000\n",
      "Epoch: 74, trainingloss: 0.0139400000,entirevalidationloss: 0.0186300000\n",
      "Epoch: 75, trainingloss: 0.0139400000,entirevalidationloss: 0.0186200000\n",
      "Epoch: 76, trainingloss: 0.0139400000,entirevalidationloss: 0.0186200000\n",
      "Epoch: 77, trainingloss: 0.0139400000,entirevalidationloss: 0.0186200000\n",
      "Epoch: 78, trainingloss: 0.0139400000,entirevalidationloss: 0.0186200000\n",
      "Epoch: 79, trainingloss: 0.0139300000,entirevalidationloss: 0.0186200000\n",
      "Epoch: 80, trainingloss: 0.0139300000,entirevalidationloss: 0.0186100000\n",
      "Epoch: 81, trainingloss: 0.0139300000,entirevalidationloss: 0.0186100000\n",
      "Epoch: 82, trainingloss: 0.0139300000,entirevalidationloss: 0.0186100000\n",
      "Epoch: 83, trainingloss: 0.0139300000,entirevalidationloss: 0.0186100000\n",
      "Epoch: 84, trainingloss: 0.0139200000,entirevalidationloss: 0.0186100000\n",
      "Epoch: 85, trainingloss: 0.0139200000,entirevalidationloss: 0.0186000000\n",
      "Epoch: 86, trainingloss: 0.0139200000,entirevalidationloss: 0.0186000000\n",
      "Epoch: 87, trainingloss: 0.0139200000,entirevalidationloss: 0.0186000000\n",
      "Epoch: 88, trainingloss: 0.0139200000,entirevalidationloss: 0.0186000000\n",
      "Epoch: 89, trainingloss: 0.0139100000,entirevalidationloss: 0.0186000000\n",
      "Epoch: 90, trainingloss: 0.0139100000,entirevalidationloss: 0.0185900000\n",
      "Epoch: 91, trainingloss: 0.0139100000,entirevalidationloss: 0.0185900000\n",
      "Epoch: 92, trainingloss: 0.0139100000,entirevalidationloss: 0.0185900000\n",
      "Epoch: 93, trainingloss: 0.0139100000,entirevalidationloss: 0.0185900000\n",
      "Epoch: 94, trainingloss: 0.0139000000,entirevalidationloss: 0.0185900000\n",
      "Epoch: 95, trainingloss: 0.0139000000,entirevalidationloss: 0.0185800000\n",
      "Epoch: 96, trainingloss: 0.0139000000,entirevalidationloss: 0.0185800000\n",
      "Epoch: 97, trainingloss: 0.0139000000,entirevalidationloss: 0.0185800000\n",
      "Epoch: 98, trainingloss: 0.0139000000,entirevalidationloss: 0.0185800000\n",
      "Epoch: 99, trainingloss: 0.0139000000,entirevalidationloss: 0.0185800000\n",
      "('Timetaken:', 5.561528205871582)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "EPOCHS = 100\n",
    "lr = 0.01\n",
    "loss_summary = []\n",
    "import time\n",
    "start = time.time()\n",
    "val_data = Variable(torch.from_numpy(testX))\n",
    "for epoch in range(0, EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    o=[]\n",
    "    for x_batch in next_batch(trainx,batch_size):\n",
    "        inputs = Variable(torch.from_numpy(x_batch))\n",
    "        target = Variable(torch.from_numpy(x_batch))\n",
    "        out = net(inputs)\n",
    "        loss = criterion(out,target)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in net.parameters():\n",
    "             param.data -= lr * param.grad.data\n",
    "    testout = net(val_data)\n",
    "    val_loss_entire = criterion(testout, val_data)\n",
    "    val_loss_entire = np.round(val_loss_entire.data.numpy()[0],5)\n",
    "    training_loss = np.round(loss.data.numpy()[0],5)\n",
    "    loss_summary.append([training_loss,val_loss_entire])\n",
    "    print(\"Epoch: {}, trainingloss: {:.10f},entirevalidationloss: {:.10f}\".format(epoch, training_loss,val_loss_entire))\n",
    "end  = time.time()\n",
    "tt = end - start\n",
    "print(\"Timetaken:\",tt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can also tweak our learning late for faster learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
